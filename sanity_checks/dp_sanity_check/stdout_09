Namespace(dataset_name=None, dataset_config_name=None, train_file='/storage/ukp/work/matzken/fplm/ft_gpt2/experiments/data/original-train.txt', validation_file='/storage/ukp/work/matzken/fplm/ft_gpt2/experiments/data/original-test.txt', validation_split_percentage=5, model_name_or_path='gpt2-medium', config_name=None, tokenizer_name='gpt2-medium', use_slow_tokenizer=False, do_ref_model=True, add_dp=True, per_device_train_batch_size=2, per_device_eval_batch_size=1, learning_rate=1e-05, weight_decay=0.01, num_train_epochs=5, max_train_steps=None, gradient_accumulation_steps=128, eval_steps=1000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, num_warmup_steps=0, output_dir='/storage/ukp/work/matzken/fplm/ft_gpt2/experiments/sanity_check_2', seed=1234, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=True, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, train_head_only=False, train_layer_n_only=None, lora_dim=4, lora_dropout=0.0, lora_alpha=32, noise_multiplier=0.9, objective='dp', per_sample_max_grad_norm=0.0, target_epsilon=None, dropout_debias=False)
model_params (million) 0.393216
model_params (million) 0.393216
cuda:0
model_params (million) 0.393216
training epoch 0
*************end of epoch 0 eval 
threshold_ref is:  0.9998154044151306
threshold is:  4.179086208343506
saving model here at step 489 and epoch 0 with ppl 91.32793854614295
correct cnt  ref is:  5678 all is:  124923 ratio is:  0.045451998431033515
correct cnt is:  10916 all is:  124923 ratio is:  0.08738182720555862
epoch 0: perplexity: 91.32793854614295 perplexity_train: 89.71250465483578
____
0.045451998431033515
0.08738182720555862
91.32793854614295
89.71250465483578
0.04588389740962505
0.08638852422272741
0.31802041722148244
0.46750996361115926
_____
End of epoch 0, we have epsilon 1.027491387925015 for alpha 9.7 from privacy engine
training epoch 1
*************end of epoch 1 eval 
threshold_ref is:  0.9996503591537476
threshold is:  4.178621292114258
saving model here at step 978 and epoch 1 with ppl 91.27151705477574
correct cnt  ref is:  6608 all is:  124923 ratio is:  0.052896584295926294
correct cnt is:  10922 all is:  124923 ratio is:  0.08742985679178374
epoch 1: perplexity: 91.27151705477574 perplexity_train: 89.65913327366539
____
0.052896584295926294
0.08742985679178374
91.27151705477574
89.65913327366539
0.053600589158208194
0.08642054369056386
0.35264377501579947
0.4676022176022176
_____
End of epoch 1, we have epsilon 1.071081148968845 for alpha 9.6 from privacy engine
training epoch 2
step 1000 epoch 2 perplexity: 91.27478123695481
*************end of epoch 2 eval 
threshold_ref is:  0.9995176196098328
threshold is:  4.178195476531982
saving model here at step 1467 and epoch 2 with ppl 91.22674426121415
correct cnt  ref is:  6948 all is:  124923 ratio is:  0.055618260848682786
correct cnt is:  10916 all is:  124923 ratio is:  0.08738182720555862
epoch 2: perplexity: 91.22674426121415 perplexity_train: 89.61613426460808
____
0.055618260848682786
0.08738182720555862
91.22674426121415
89.61613426460808
0.05641830232781531
0.08635650475489097
0.3644260599793175
0.4674176776429809
_____
End of epoch 2, we have epsilon 1.1075117867454527 for alpha 9.5 from privacy engine
training epoch 3
*************end of epoch 3 eval 
threshold_ref is:  0.9993999600410461
threshold is:  4.177821636199951
saving model here at step 1956 and epoch 3 with ppl 91.18751542617942
correct cnt  ref is:  7210 all is:  124923 ratio is:  0.0577155527805128
correct cnt is:  10916 all is:  124923 ratio is:  0.08738182720555862
epoch 3: perplexity: 91.18751542617942 perplexity_train: 89.57892215697363
____
0.0577155527805128
0.08738182720555862
91.18751542617942
89.57892215697363
0.058659665076366434
0.08638852422272741
0.37349643221202855
0.46750996361115926
_____
End of epoch 3, we have epsilon 1.1413987145640725 for alpha 9.4 from privacy engine
training epoch 4
saving model here at step 2000 and epoch 4 with ppl 91.18273257608661
step 2000 epoch 4 perplexity: 91.18273257608661
*************end of epoch 4 eval 
threshold_ref is:  0.9992649555206299
threshold is:  4.177433490753174
saving model here at step 2440 and epoch 4 with ppl 91.14095852630781
correct cnt  ref is:  7354 all is:  124923 ratio is:  0.058868262849915545
correct cnt is:  10922 all is:  124923 ratio is:  0.08742985679178374
epoch 4: perplexity: 91.14095852630781 perplexity_train: 89.53399770997389
____
0.058868262849915545
0.08742985679178374
91.14095852630781
89.53399770997389
0.05974832698280554
0.0864525631584003
0.377809273132213
0.46769443963277324
_____
End of epoch 4, we have epsilon 1.1719704246225753 for alpha 9.4 from privacy engine
*************end of training 
threshold_ref is:  0.9992649555206299
threshold is:  4.177433490753174
end of training epsilon privacy engine 1.1719704246225753
correct cnt  ref is:  7354 all is:  124923 ratio is:  0.058868262849915545
correct cnt is:  10922 all is:  124923 ratio is:  0.08742985679178374
end of training perplexity: 91.14095852630781 perplexity_train: 89.53399770997389
____
0.058868262849915545
0.08742985679178374
91.14095852630781
89.53399770997389
0.05974832698280554
0.0864525631584003
0.377809273132213
0.46769443963277324
_____
