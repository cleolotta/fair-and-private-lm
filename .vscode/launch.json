{
    "version": "0.2.0",
    "configurations": [

        {
            "name": "Python: Aktuelle Datei",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "env": { "PYTHONPATH": "${workspaceRoot}"},
            "cwd": "${workspaceRoot}",
            "console": "integratedTerminal",
            "args": ["--model_name_or_path", "gpt2", "--tokenizer_name", "gpt2", "--train_file", "./test/augmented-train.txt","--mia_train_file", "./test/original-mia-train.txt", "--validation_file", "./test/augmented-test.txt", "--mia_validation_file", "./test/original-mia-test.txt", "--block_size", "1024", "--output_dir" ,"C:/Users/cmatz/master-thesis/fair-and-private-lm/test", "--eval_steps", "100", "--learning_rate", "1e-5", "--do_ref_model", "--per_device_eval_batch_size", "1", "--per_device_train_batch_size", "1", "--gradient_accumulation_steps", "1", "--num_train_epochs", "2", "--lora_dim", "4", "--lora_alpha", "32", "--lora_dropout", "0.0", "--noise_multiplier", "0.6"]
        }
    ]
}