Namespace(dataset_name=None, dataset_config_name=None, train_file='/storage/ukp/work/matzken/fplm/datasets/original-train1.txt', validation_file='/storage/ukp/work/matzken/fplm/datasets/original-test1.txt', validation_split_percentage=5, model_name_or_path='gpt2-medium', config_name=None, tokenizer_name='gpt2-medium', use_slow_tokenizer=False, do_ref_model=True, add_dp=True, per_device_train_batch_size=2, per_device_eval_batch_size=1, learning_rate=1e-05, weight_decay=0.01, num_train_epochs=2, max_train_steps=None, gradient_accumulation_steps=128, eval_steps=1000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, num_warmup_steps=0, output_dir='/storage/ukp/work/matzken/fplm/ft_gpt2/experiments/models', seed=1234, model_type=None, block_size=1024, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, train_head_only=False, train_layer_n_only=None, lora_dim=4, lora_dropout=0.0, lora_alpha=32, per_sample_max_grad_norm=0.0, noise_multiplier=0.6, objective='dp', dropout_debias=False)
------------------------------------ make sure the layers are frozen ---------------------------------------------------------------
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
False
False
True
True
False
False
False
False
False
False
False
False
False
False
----------------------------- make sure the layers of ref model are not frozen -----------------------------------------------------
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
model_params (million) 0.393216
cuda:0
model_params (million) 0.393216
training epoch 0
saving model here at step 1000 and epoch 0 with ppl 37.83273215011085
step 1000 epoch 0 perplexity: 37.83273215011085
*************end of epoch 0 eval 
threshold_ref is:  0.9988846182823181
threshold is:  3.200674533843994
saving model here at step 1974 and epoch 0 with ppl 37.77587658908929
correct cnt  ref is:  16744 all is:  505046 ratio is:  0.0331534157284683
correct cnt is:  18164 all is:  505046 ratio is:  0.03596504080816401
epoch 0: perplexity: 37.77587658908929 perplexity_train: 37.90423824830078
____
0.0331534157284683
0.03596504080816401
37.77587658908929
37.90423824830078
0.033446325893776434
0.036416340625049504
0.2535422670509126
0.2699782749104574
_____
End of epoch 0, we have epsilon 2.5752079100308336 for alpha 5.3 from privacy engine
training epoch 1
saving model here at step 2000 and epoch 1 with ppl 37.77275147298867
step 2000 epoch 1 perplexity: 37.77275147298867
saving model here at step 3000 and epoch 1 with ppl 37.732328403829904
step 3000 epoch 1 perplexity: 37.732328403829904
